---
layout:            post
title:             "2. Hadoop2 완전분산모드 세팅하기(HA, 주키퍼)"
menutitle:         "2. Hadoop2 완전분산모드 세팅하기(HA, 주키퍼)"
category:          BigData
author:            geunyoung
cover:             /assets/mountain-alternative-cover.jpg
published:         true
language:          KO
comments:          true
---
 
  
### 준비물 및 환경
  
 * VMWare 4대 (https://my.vmware.com/web/vmware/downloads 에서 VMware Workstation Player 다운로드) 
  
 * CentOS7 (http://mirror.kakao.com/centos/7.7.1908/isos/x86_64/ 에서 Centos-7-x86_64-DVD-1908.iso 다운로드)
  
 * Java1.7 (https://www.oracle.com/java/technologies/javase-downloads.html 에서 필요한 버전 다운로드)
  
 * Hadoop2.7 (http://www.apache.org/dyn/closer.cgi/hadoop/common 에서 필요한 버전 다운로드)
 
 * Protocol Buffer2.5.0 (https://github.com/google/protobuf/releases/download/v2.5.0/protobuf-2.5.0.tar.gz)
 
 * zookeeper3.4 (http://apache.mirror.cdnetworks.com/zookeeper/zookeeper-3.4.14/zookeeper-3.4.14.tar.gz)
  
### 세팅에 앞서...
  
실제 업무에서는 이번 장에서 다룰 완전분산 모드로 시스템을 구축한다. 이유는 2개의 namenode를 통해 한개의 namenode가 이상이 있더라도 다른 namenode를 통해 서비스할 수 있으며, 다수의 datanode에 백업 데이터를 만들어 하나의 datanode가 고장나더라도 data 손실이나 서비스 중단을 막을 수 있기 때문이다.
  
분산 환경은 Sever1에 주키퍼, 액티브 네임노트, 저널노드, 데이터노드, Server2에 주키퍼, 스탠바이 네임노드, 저널노드, 데이터노드, Server3에 주키퍼, 저널노드, 데이터노드, Server4에 데이터노드를 설정하는 작업을 할 것이다.
  
### 1장 이어서 완전분산 모드로 세팅하기
  
1. 이전 장에서는 사용하는 포트가 늘어날 때마다 해당 포트를 열어주었는데, 일반적으로 시스템을 운영할 때 OS에서 방화벽을 막는 것이 아닌 L4 스위치 등 더 앞단에서 방화벽 처리를 한다고 한다.각 서버마다 OS에서의 방화벽 해제를 하는 작업부터 진행해보자.
```text
systemctl disable firewalld
아래의 설정은 centOS 6부터는 사용되지 않는다고 하는데 오픈소스다보니 아래의 설정때문에 오류가 났던 적이 있다고 하여 설정하였다.
vi /etc/selinux/config
SELINUX=disabled로 수정
```
  
2. 시간 동기화 또한 이전 장에서 해야 했던 내용인데 클러스터링으로 사용할 시 제일 먼저 세팅해야하는 내용이다.
```text
yum install ntp -y
vi /etc/ntp.conf
이전 IP 주석 후,
server 0.asia.pool.ntp.org
server 1.asia.pool.ntp.org
server 2.asia.pool.ntp.org
server 3.asia.pool.ntp.org
systemctl start ntpd <--시작
systemctl enable ntpd <--재부팅시에도 시작하기
ntpq -p <-- 작동여부 확인
```
  
3. 이제 주키퍼를 다운 및 세팅할 것이다. 우선 1,2,3서버에 주키퍼용 계정 생성
```text
useradd zookeeper
passwd zookeeper
```
  
4. 1번 서버 주키퍼 계정으로 생성하여 /home/zookeeper의 위치에 주키퍼.tar.gz다운 
```text
wget http://apache.mirror.cdnetworks.com/zookeeper/zookeeper-3.4.14/zookeeper-3.4.14.tar.gz
tar xvfz zookeeper-3.4.14.tar.gz
```
  
5. conf에 있는 zoo_sample.cfg를 활용하여 zoo.cfg작성
```text
cp conf/zoo_sample conf/zoo.cfg
vi conf/zoo.cfg
dataDir=/home/zookeeper/data로 수정
server.1=server1:2888:3888
server.2=server2:2888:3888
server.3=server3:2888:3888
추가
```
설명을 좀 덧붙이자면 dataDir은 주키퍼 스냅샷을 저장하는 경로, maxClientCnxns는 클리언트풀, maxSessisonTimeout은 세션시간, server.#는 서버들의 아이디이다. 2888은 주키퍼 리더에 접속하기 위한 포트, 3888은 리더 결정을 위한 포트이다.
  
6. 이제 설정한 아이디를 등록할 것이다.
```text
vi data/myid
1
```
  
7. zookeeper관련된 ssh키 설정 및 전달
```text
ssh-keygen -t rsa -b 4096 -C "본인이메일주소"
이름은 hadoop의 키와 다르게
ssh-copy-id -i /home/hadoop/.ssh/다르게만든이름.pub hadoop@server1
ssh-copy-id -i /home/hadoop/.ssh/다르게만든이름.pub hadoop@server2
ssh-copy-id -i /home/hadoop/.ssh/다르게만든이름.pub hadoop@server3
ssh-copy-id -i /home/hadoop/.ssh/다르게만든이름.pub hadoop@server4
정확히는 모르겠는데 하둡실행시 server1에서 server1으로 갈때도 ssh를 통해 가는 로직이 있나봄. 그래서 자기 자신의 키도 넣음
```

8. zookeeper 설정을 서버2,3에도 옮기는 작업 진행
```text
tar cvgz zookeeper.tar.gz zookeeper-3.4.14
scp zookeeper.tar.gz zookeeper@server2:/home/zookeeper
scp zookeeper.tar.gz zookeeper@server3:/home/zookeeper
ssh server2 "cd /home/zookeeper; tar xvfz zookeeper.tar.gz;mkdir data"
ssh server3 "cd /home/zookeeper; tar xvfz zookeeper.tar.gz;mkdir data"
```
  
9. 서버2,3에도 아이디 만들어 주기
```text
서버2에서
vi data/myid
2
서버3에서
vi data/myid
3
```
  
10. 서버1,2,3에 주키퍼 실행 및 상태 확인
```text
./bin/zkServer.sh start
./bin/zkServer.sh status
```
  
11. 이제 하둡세팅을 진행할 것이다. 우선 masters 삭제.  
스탠바이 네임노드가 보조네임노드의 역할을 할 수 있어서 필요가 없어진다.
  
12. slaves 수정
```text
#localhost
server1
server2
server3
server4
```
  
13. core-site.xml 수정(Git 블로그 에러로 샘플 아래에 작성)  
속성 정리를 해보자면  
dfs.namenode.name.dir = 네임 노드의 데이터 저장할 위치  
dfs.datanode.data.dir = 데이터 노드의 데이터 저장할 위치  
dfs.journalnode.edits.dir = 저널 노드의 데이터 저장할 위치  
dfs.nameservices = 네임 노드의 기능 설정. 일반적으로 클러스터링 세팅할 때 사용  
dfs.ha.namenodes.클러스터그룹명 = nameservices에 설정하려면 클러스터 명 설정  
dfs.namenode.rpc-address.클러스터그룹명.클러스터명 = rpc에 대한 클러스터명이 어디 서버와 포트를 사용할 것인지. rpc란 액티브, 스탠바이 관련 데이터인듯    
dfs.namenode.http-address.클러스터그룹명.클러스터명 = http에 대한 클러스터명이 어디 서버와 포트를 사용할 것인지. 웹 어드민 화면을 뿌릴 때 사용.
dfs.namenode.shared.edits.dir = 저널 노드에 대한 어디 서버와 포트하는지  
dfs.client.failover.proxy.provider.클러스터그룹명 = 액티브, 스탠바이 액션을 어떤 로직으로 돌릴지  
dfs.ha.fencing.methods = 액티브에서 스탠바이로 전환되면 그 노드는 접근 차단 되어야하는데 관련 로직에 대한 세팅  
dfs.ha.ssh.private-key-files = HA 왔다갔다 할때 어떤 키파일 사용할 것인지  
dfs.ha.automatic-failover.enabled = 장애발생시 액티브, 스탠바이 전환 할 것인지  
  
14. hdfs-site.xml 수정(Git 블로그 에러로 샘플 아래에 작성)
  
15. hadoop-env.sh, mapred-site,xml, yarn-env.sh, yarn-site.xml은 이전 1장와 같이 설정
  
16. 각 서버에 dfs.namenode.name.dir, dfs.datanode.data.dir, journalnode.edits.dir에 설정한 디렉토리 만들어 주기
  
17. 위의 세팅 server2,3,4에게 전달
```text
tar cvfz hadoop.tar.gz hadoop-2.7.7
scp hadoop.tar.gz hadoop@server2:/home/hadoop
scp hadoop.tar.gz hadoop@server3:/home/hadoop
scp hadoop.tar.gz hadoop@server4:/home/hadoop
ssh hadoop@server2 "cd /home/hadoop; tar xvfz hadoop.tar.gz;"
ssh hadoop@server3 "cd /home/hadoop; tar xvfz hadoop.tar.gz;"
ssh hadoop@server4 "cd /home/hadoop; tar xvfz hadoop.tar.gz;"
```
  
18. 주키퍼 초기화
```text
./bin/hdfs zkfc -formatZK
```
  
19. 저널노드 실행  
각 서버1,2,3에서 아래의 명령어 실행
```text
./sbin/hadoop-daemon.sh start journalnode
```
  
20. 네임노드 포맷
```text
./bin/hdfs namenode -format
```

21. 서버2의 네임노드 스탠바이로 설정  
서버2로 들어가서
```text
./bin/hdfs namenode -bootstrapStandby
```
  
22. 서버1,2로 가서 네임노드 실행
```text
./sbin/hadoop-daemon.sh start namenode
```

23. 서버1,2에서 주키퍼 컨트롤러 실행
```text
./sbin/hadoop-daemon.sh start zkfc
```
  
24. 데이터노드 실행
```text
책에서는 서버1 실행시키면 2,3,4 실행이 된다고 적혀있었지만 난 안되었음. 직접가서 2,3,4 실행 했고 문제없이 돌아가는 것 확인
./sbin/hadoop-daemon.sh start datanode
```
  
25. hdfs잘 작동하는지 디렉토리 만들며 테스트
```text
./sbin/hdfs dfs -ls /
./sbin/hdfs dfs -mkdir /user
./sbin/hdfs dfs -mkdir /user/hadoop
./sbin/hdfs dfs -mkdir /user/hadoop/conf
./sbin/hdfs dfs -put ../../../../hadoop-2.7.7/etc/hadoop/hadoop-env.sh /user/hadoop/conf/
./sbin/hdfs dfs -ls conf
```
  
26. 얀 클러스터 실행
```text
./sbin/start-yarn.sh
이것도 서버1 실행시키면 2,3 실행이 된다고 적혀있었지만 난 안되었음. 직접가서 2,3 실행 했고 문제없이 돌아가는 것 확인
```
  
27. 맵리듀스 잡을 위한 히스토이 서버 실행
```text
mr-jobhistory-daemon.sh start historyserver
```

28. jps를 통해 제대로 켜져있는지 확인
```text
서버1,2는 Jps, DFSZKFailoverController, Namenode, ResourceManager, JobHistoryServer, JournalNode, DataNode, NodeManager가 켜져있어야 하고, 서버3,4는 Jps, DataNode, NodeManager, JournalNode가 켜져 있어야한다.
```
  
29. 예제를 통한 정상 작동 확인
```text
./bin/yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar wordcount conf output
```


##### xml 샘플들
  
###### core-site.xml

```xml
<configuration>
 <property>
  <name>fs.defaultFS</name>
  <value>hdfs://servers-cluster</value>
 </property>

 <property>
  <name>ha.zookeeper.quorum</name>
  <value>server1:2181,server2:2181,server3:2181</value>
 </property>
</configuration>
```
  
###### hdfs-site.xml

```xml
<configuration>
 <property>
  <name>dfs.namenode.name.dir</name>
  <value>/home/hadoop/data/dfs/namenode</value>
 </property>

 <property>
  <name>dfs.datanode.data.dir</name>
  <value>/home/hadoop/data/dfs/datanode</value>
 </property>

 <property>
  <name>dfs.journalnode.edits.dir</name>
  <value>/home/hadoop/data/dfs/journalnode</value>
 </property>

 <property>
  <name>dfs.nameservices</name>
  <value>servers-cluster</value>
 </property>

 <property>
  <name>dfs.ha.namenodes.servers-cluster</name>
  <value>nn1,nn2</value>
 </property>

 <property>
  <name>dfs.namenode.rpc-address.servers-cluster.nn1</name>
  <value>server1:8020</value>
 </property>

 <property>
  <name>dfs.namenode.rpc-address.servers-cluster.nn2</name>
  <value>server2:8020</value>
 </property>

 <property>
  <name>dfs.namenode.http-address.servers-cluster.nn1</name>
  <value>server1:50070</value>
 </property>

 <property>
  <name>dfs.namenode.http-address.servers-cluster.nn2</name>
  <value>server2:5070</value>
 </property>

 <property>
  <name>dfs.namenode.shared.edits.dir</name>
  <value>qjournal://server1:8485;server2:8485;server3:8485/servers-cluster</value>
 </property>

 <property>
  <name>dfs.client.failover.proxy.provider.servers-cluster</name>
  <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
 </property>

 <property>
  <name>dfs.ha.fencing.methods</name>
  <value>sshfence</value>
 </property>

 <property>
  <name>dfs.ha.ssh.private-key-files</name>
  <value>/home/hadoop/.ssh/id_rsa</value>
 </property>


 <property>
  <name>dfs.ha.automatic-failover.enabled</name>
  <value>true</value>
 </property>
</configuration>
```
  
  
### 어려웠던 점...
  
오타 좀 안내야 할텐데... 오타 잡느라 세팅 시간의 4배는 더 걸린듯...
  
### 다음 장
  
다음 장에는 하둡 빌드에 대해서 다룰 예정이다.

