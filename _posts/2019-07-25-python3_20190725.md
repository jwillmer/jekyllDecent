---
layout:            post
title:             "Python Cawling 시작하기"
menutitle:         "Python Cawling 시작하기"
tags:              Python
category:          Python
author:            geunyoung
cover:             /assets/mountain-alternative-cover.jpg
published:         true
language:          KO
comments:          true
---

## Crawling 들어가기 앞서...

크롤링(또는 스크래이핑)이란 웹페이지의 내용을 그대로 가져와서 필요한 데이터를 추출해 내는 것을 말한다.

사이트 운영자의 의사에 반하여 상업적인 용도로 크롤링을 한다면 위법이라 할 수 있다.

그러니 크롤링을 상업적인 용도로 사용할 것이라면 반드시 확인하고 크롤링 하길 바란다.

일반적으로 유명 사이트들은 도메인/robots.txt에 크롤링에 관하여 허용한지 안한지 확인할 수 있으니

상업적 용도로 쓰려면 반드시 확인바란다.



### 크롤링 했을때 사용한 라이브러 및 소스 일부

request : url을 통한 데이터를 가져오고 싶을 때

bs4, BeautifulSoup : url을 통해 데이터를 가져올 때, json, html, xml 등등의 형식일 수 있는데, html 형식일 시 태그 및 속성, id를 통해 데이터를 가져오고 싶을 때

konlpy, Okt : 데이터를 부분적으로(혹은 명사) 조건에 따라 뽑아내고 싶을 때(해당 라이브러리 설치하는 데 꽤나 애를 먹었다. 추후에 설치하는 법에 대해 정리하겠음 

아래엔 해당 라이브러리 어떻게 활용했는지 부분 부분 빼온 소스이다.

```python

main_domain = '도메인'

(이하 생략)

#IT관련된 카테고리 URL 리스트로 뽑기
def get_detail_url_list(url) :
    detail_url = []

    response = requests.get(url + 'jobs/work')
    raw_html_data = BeautifulSoup(response.text, 'html.parser')

    temp_html_data1 = raw_html_data.findAll('div', {'class': 'panel'})

    temp_html_data2_1 = get_IT_div(temp_html_data1, '인터넷')
    temp_html_data2_2 = get_IT_div(temp_html_data1, '정보통신,전자,전산')

    temp_html_data3_1 = temp_html_data2_1.select('li > a')
    temp_html_data3_2 = temp_html_data2_2.select('li > a')

    for temp_data4 in temp_html_data3_1 :
        detail_url.append(temp_data4.get('href'))

    for temp_data4 in temp_html_data3_2 :
        detail_url.append(temp_data4.get('href'))

    return detail_url
    
(이하 생략)
    
#메인 화면에서 IT관련된 카테고리 뽑기
def get_IT_div(html_datas, key_word) :
    for html_data in html_datas :
        html_data_div = html_data.find('div', {'class' : 'panel-heading'})
        if html_data_div.find('a').text == key_word :
            return html_data


(이하 생략)

#main_domin : 컨텍스트루트, detail_list : IT->웹개발자,앱개발자,퍼블리셔 등등 소분류 카테고리 접근 URL
def start_crawling(main_domain, detail_url_list) :

    #상세 공고페이지 URL 담아낼 리스트
    detail_detail_url_list = []

    for detail_url in detail_url_list :

        page_count = 1

        #다음 페이지라는 버튼이 비화성화 될때까지 루프
        while True:
            print("requesting url >>>>>> " + main_domain + detail_url + '&page=' + str(page_count))
            response = requests.get(main_domain + detail_url + '&page=' + str(page_count))
            raw_html_data = BeautifulSoup(response.text, 'html.parser')

            temp_html_data1 = raw_html_data.findAll('td',{'class':'job-title'})

            for temp_html_data2 in temp_html_data1 :
                temp_html_data3 = temp_html_data2.find('a').get('href')
                detail_detail_url_list.append(temp_html_data3)

            if not raw_html_data.find('ul',{'class':'pagination'}) :
                print("There isn't another page..... moving to next category")
                break

            last_page_check = raw_html_data.find('ul',{'class':'pagination'}).findAll('li')
            if last_page_check[len(last_page_check) - 1].find('a') :
                page_count += 1
            elif page_count > 100 : #무한루프 풀기위한 방어코드
                print('something went wrong')
                break
            else :
                print('Reached last page count >>> ' + str(page_count) + "..... moving to next category")
                break

    print(detail_detail_url_list)
	    
```



### 느낀점 및 실수했던 부분

정말 직관적인 느낌의 순차적으로 실행되는

아직까진 내가봐도 참 허접한 소스이다.

python에도 객체화 개념(아마 라이브러리화 시키는 느낌이었음)이 있던데

추후에 깔끔하게 소스를 정리하고 화 예정이다.


